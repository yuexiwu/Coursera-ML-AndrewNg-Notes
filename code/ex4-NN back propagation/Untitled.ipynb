{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基础包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib\n",
    "import scipy.optimize as opt\n",
    "from sklearn.metrics import classification_report  # 这个包是评价报告\n",
    "\n",
    "\n",
    "def load_data(path, transpose=True):\n",
    "    data = sio.loadmat(path)\n",
    "    y = data.get('y')  # (5000,1)\n",
    "    y = y.reshape(y.shape[0])  # make it back to column vector\n",
    "\n",
    "    X = data.get('X')  # (5000,400)\n",
    "\n",
    "    if transpose:\n",
    "        # for this dataset, you need a transpose to get the orientation right\n",
    "        X = np.array([im.reshape((20, 20)).T for im in X])\n",
    "\n",
    "        # and I flat the image again to preserve the vector presentation\n",
    "        X = np.array([im.reshape(400) for im in X])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def plot_100_image(X):\n",
    "    \"\"\" sample 100 image and show them\n",
    "    assume the image is square\n",
    "\n",
    "    X : (5000, 400)\n",
    "    \"\"\"\n",
    "    size = int(np.sqrt(X.shape[1]))\n",
    "\n",
    "    # sample 100 image, reshape, reorg it\n",
    "    sample_idx = np.random.choice(np.arange(X.shape[0]), 100)  # 100*400\n",
    "    sample_images = X[sample_idx, :]\n",
    "\n",
    "    fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8))\n",
    "\n",
    "    for r in range(10):\n",
    "        for c in range(10):\n",
    "            ax_array[r, c].matshow(sample_images[10 * r + c].reshape((size, size)),\n",
    "                                   cmap=matplotlib.cm.binary)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "\n",
    "\n",
    "def expand_y(y):\n",
    "    #     \"\"\"expand 5000*1 into 5000*10\n",
    "    #     where y=10 -> [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]: ndarray\n",
    "    #     \"\"\"\n",
    "    res = []\n",
    "    for i in y:\n",
    "        y_array = np.zeros(10)\n",
    "        y_array[i - 1] = 1\n",
    "\n",
    "        res.append(y_array)\n",
    "\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse=False)\n",
    "# y_onehot = encoder.fit_transform(y)\n",
    "# y_onehot.shape #这个函数与expand_y(y)一致\n",
    "\n",
    "def load_weight(path):\n",
    "    data = sio.loadmat(path)\n",
    "    return data['Theta1'], data['Theta2']\n",
    "\n",
    "\n",
    "def serialize(a, b):\n",
    "    return np.concatenate((np.ravel(a), np.ravel(b)))\n",
    "\n",
    "\n",
    "# 序列化2矩阵\n",
    "# 在这个nn架构中，我们有theta1（25,401），theta2（10,26），它们的梯度是delta1，delta2\n",
    "def feed_forward(theta, X):\n",
    "    \"\"\"apply to architecture 400+1 * 25+1 *10\n",
    "    X: 5000 * 401\n",
    "    \"\"\"\n",
    "\n",
    "    t1, t2 = deserialize(theta)  # t1: (25,401) t2: (10,26)\n",
    "    m = X.shape[0]\n",
    "    a1 = X  # 5000 * 401\n",
    "\n",
    "    z2 = a1 @ t1.T  # 5000 * 25\n",
    "    a2 = np.insert(sigmoid(z2), 0, np.ones(m), axis=1)  # 5000*26\n",
    "\n",
    "    z3 = a2 @ t2.T  # 5000 * 10\n",
    "    h = sigmoid(z3)  # 5000*10, this is h_theta(X)\n",
    "\n",
    "    return a1, z2, a2, z3, h  # you need all those for backprop\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def deserialize(seq):\n",
    "    #     \"\"\"into ndarray of (25, 401), (10, 26)\"\"\"\n",
    "    return seq[:25 * 401].reshape(25, 401), seq[25 * 401:].reshape(10, 26)\n",
    "\n",
    "\n",
    "def cost(theta, X, y):\n",
    "    #     \"\"\"calculate cost\n",
    "    #     y: (m, k) ndarray\n",
    "    #     \"\"\"\n",
    "    m = X.shape[0]  # get the data size m\n",
    "\n",
    "    _, _, _, _, h = feed_forward(theta, X)\n",
    "\n",
    "    # np.multiply is pairwise operation\n",
    "    pair_computation = -np.multiply(y, np.log(h)) - np.multiply((1 - y), np.log(1 - h))\n",
    "\n",
    "    return pair_computation.sum() / m\n",
    "\n",
    "\n",
    "def regularized_cost(theta, X, y, l=1):\n",
    "    \"\"\"the first column of t1 and t2 is intercept theta, ignore them when you do regularization\"\"\"\n",
    "    t1, t2 = deserialize(theta)  # t1: (25,401) t2: (10,26)\n",
    "    m = X.shape[0]\n",
    "\n",
    "    reg_t1 = (l / (2 * m)) * np.power(t1[:, 1:], 2).sum()  # this is how you ignore first col\n",
    "    reg_t2 = (l / (2 * m)) * np.power(t2[:, 1:], 2).sum()\n",
    "\n",
    "    return cost(theta, X, y) + reg_t1 + reg_t2\n",
    "\n",
    "\n",
    "def sigmoid_gradient(z):\n",
    "    \"\"\"\n",
    "    pairwise op is key for this to work on both vector and matrix\n",
    "    \"\"\"\n",
    "    return np.multiply(sigmoid(z), 1 - sigmoid(z))\n",
    "\n",
    "\n",
    "def gradient(theta, X, y):\n",
    "    # initialize\n",
    "    t1, t2 = deserialize(theta)  # t1: (25,401) t2: (10,26)\n",
    "    m = X.shape[0]\n",
    "\n",
    "    delta1 = np.zeros(t1.shape)  # (25, 401)\n",
    "    delta2 = np.zeros(t2.shape)  # (10, 26)\n",
    "\n",
    "    a1, z2, a2, z3, h = feed_forward(theta, X)\n",
    "\n",
    "    for i in range(m):\n",
    "        a1i = a1[i, :]  # (1, 401)\n",
    "        z2i = z2[i, :]  # (1, 25)\n",
    "        a2i = a2[i, :]  # (1, 26)\n",
    "\n",
    "        hi = h[i, :]  # (1, 10)\n",
    "        yi = y[i, :]  # (1, 10)\n",
    "\n",
    "        d3i = hi - yi  # (1, 10)\n",
    "\n",
    "        z2i = np.insert(z2i, 0, np.ones(1))  # make it (1, 26) to compute d2i\n",
    "        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))  # (1, 26)\n",
    "\n",
    "        # careful with np vector transpose\n",
    "        delta2 += np.matrix(d3i).T @ np.matrix(a2i)  # (1, 10).T @ (1, 26) -> (10, 26)\n",
    "        delta1 += np.matrix(d2i[1:]).T @ np.matrix(a1i)  # (1, 25).T @ (1, 401) -> (25, 401)\n",
    "\n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "\n",
    "    return serialize(delta1, delta2)\n",
    "\n",
    "\n",
    "def gradient_checking(theta, X, y, epsilon, regularized=False):\n",
    "    def a_numeric_grad(plus, minus, regularized=False):\n",
    "        \"\"\"calculate a partial gradient with respect to 1 theta\"\"\"\n",
    "        if regularized:\n",
    "            return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * 2)\n",
    "        else:\n",
    "            return (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * 2)\n",
    "\n",
    "    theta_matrix = expand_array(theta)  # expand to (10285, 10285)\n",
    "    epsilon_matrix = np.identity(len(theta)) * epsilon\n",
    "\n",
    "    plus_matrix = theta_matrix + epsilon_matrix\n",
    "    minus_matrix = theta_matrix - epsilon_matrix\n",
    "\n",
    "    # calculate numerical gradient with respect to all theta\n",
    "    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)\n",
    "                             for i in range(len(theta))])\n",
    "\n",
    "    # analytical grad will depend on if you want it to be regularized or not\n",
    "    analytic_grad = regularized_gradient(theta, X, y) if regularized else gradient(theta, X, y)\n",
    "\n",
    "    # If you have a correct implementation, and assuming you used EPSILON = 0.0001\n",
    "    # the diff below should be less than 1e-9\n",
    "    # this is how original matlab code do gradient checking\n",
    "    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)\n",
    "\n",
    "    print(\n",
    "        'If your backpropagation implementation is correct,\\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\\nRelative Difference: {}\\n'.format(\n",
    "            diff))\n",
    "\n",
    "\n",
    "def expand_array(arr):\n",
    "    \"\"\"replicate array into matrix\n",
    "    [1, 2, 3]\n",
    "\n",
    "    [[1, 2, 3],\n",
    "     [1, 2, 3],\n",
    "     [1, 2, 3]]\n",
    "    \"\"\"\n",
    "    # turn matrix back to ndarray\n",
    "    return np.array(np.matrix(np.ones(arr.shape[0])).T @ np.matrix(arr))\n",
    "\n",
    "\n",
    "def regularized_gradient(theta, X, y, l=1):\n",
    "    \"\"\"don't regularize theta of bias terms\"\"\"\n",
    "    m = X.shape[0]\n",
    "    delta1, delta2 = deserialize(gradient(theta, X, y))\n",
    "    t1, t2 = deserialize(theta)\n",
    "\n",
    "    t1[:, 0] = 0\n",
    "    reg_term_d1 = (l / m) * t1\n",
    "    delta1 = delta1 + reg_term_d1\n",
    "\n",
    "    t2[:, 0] = 0\n",
    "    reg_term_d2 = (l / m) * t2\n",
    "    delta2 = delta2 + reg_term_d2\n",
    "\n",
    "    return serialize(delta1, delta2)\n",
    "\n",
    "\n",
    "def random_init(size):\n",
    "    return np.random.uniform(-0.12, 0.12, size)\n",
    "\n",
    "\n",
    "def nn_training(X, y):\n",
    "    \"\"\"regularized version\n",
    "    the architecture is hard coded here... won't generalize\n",
    "    \"\"\"\n",
    "    init_theta = random_init(10285)  # 25*401 + 10*26\n",
    "\n",
    "    res = opt.minimize(fun=regularized_cost,\n",
    "                       x0=init_theta,\n",
    "                       args=(X, y, 1),\n",
    "                       method='TNC',\n",
    "                       jac=regularized_gradient,\n",
    "                       options={'maxiter': 400})\n",
    "    return res\n",
    "\n",
    "\n",
    "def show_accuracy(theta, X, y):\n",
    "    _, _, _, _, h = feed_forward(theta, X)\n",
    "\n",
    "    y_pred = np.argmax(h, axis=1) + 1\n",
    "\n",
    "    print(classification_report(y, y_pred))\n",
    "\n",
    "\n",
    "def plot_hidden_layer(theta):\n",
    "    \"\"\"\n",
    "    theta: (10285, )\n",
    "    \"\"\"\n",
    "    final_theta1, _ = deserialize(theta)\n",
    "    hidden_layer = final_theta1[:, 1:]  # ger rid of bias term theta\n",
    "\n",
    "    fig, ax_array = plt.subplots(nrows=5, ncols=5, sharey=True, sharex=True, figsize=(5, 5))\n",
    "\n",
    "    for r in range(5):\n",
    "        for c in range(5):\n",
    "            ax_array[r, c].matshow(hidden_layer[5 * r + c].reshape((20, 20)),\n",
    "                                   cmap=matplotlib.cm.binary)\n",
    "            plt.xticks(np.array([]))\n",
    "            plt.yticks(np.array([]))\n",
    "\n",
    "# nn functions starts here ---------------------------\n",
    "# ps. all the y here is expanded version (5000,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行\n",
    "from back_propagation.core import *\n",
    "\n",
    "X, _ = load_data('ex4data1.mat')\n",
    "plot_100_image(X)\n",
    "plt.show()\n",
    "\n",
    "X_raw, y_raw = load_data('ex4data1.mat', transpose=False)\n",
    "X = np.insert(X_raw, 0, np.ones(X_raw.shape[0]), axis=1)  # 增加全部为1的一列\n",
    "print(X.shape)\n",
    "print(y_raw)\n",
    "\n",
    "y = expand_y(y_raw)\n",
    "t1, t2 = load_weight('ex4weights.mat')\n",
    "print(t1.shape, t2.shape)\n",
    "\n",
    "theta = serialize(t1, t2)  # 扁平化参数，25*401+10*26=10285\n",
    "print(theta.shape)\n",
    "\n",
    "_, _, _, _, h = feed_forward(theta, X)\n",
    "print(h)\n",
    "# 5000*10\n",
    "print(cost(theta, X, y))\n",
    "\n",
    "print(regularized_cost(theta, X, y))\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(t1.shape, t2.shape)\n",
    "print(theta.shape)\n",
    "\n",
    "print(sigmoid_gradient(0))\n",
    "\n",
    "d1, d2 = deserialize(gradient(theta, X, y))\n",
    "print(d1.shape, d2.shape)\n",
    "\n",
    "gradient_checking(theta, X, y, epsilon=0.0001)\n",
    "# 这个运行很慢，谨慎运行\n",
    "gradient_checking(theta, X, y, epsilon=0.0001, regularized=True)\n",
    "# 这个运行很慢，谨慎运行\n",
    "\n",
    "res = nn_training(X, y)  # 慢\n",
    "print(res)\n",
    "\n",
    "_, y_answer = load_data('ex4data1.mat')\n",
    "print(y_answer[:20])\n",
    "final_theta = res.x\n",
    "\n",
    "plot_hidden_layer(final_theta)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
